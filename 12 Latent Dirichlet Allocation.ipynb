{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12 Latent Dirichlet Allocation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP0nIUvXOA+bU0Z9RDOYIK6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jisang93/NLP_Class/blob/master/12%20Latent%20Dirichlet%20Allocation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApCTZQ-HHDTR",
        "colab_type": "text"
      },
      "source": [
        "# **Topic Modeling - Latent Dirichlet Allocation 실습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29eDj3kOHMbN",
        "colab_type": "text"
      },
      "source": [
        "## **1. 잠재 디리클레 할당(LDA)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZLex5ycVb-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs_ls = [\"Cute kitty\",\n",
        "          \"Eat rice or cake\",\n",
        "          \"Kitty and hamster\",\n",
        "          \"Eat bread\",\n",
        "          \"Rice, bread and cake\",\n",
        "          \"Cute hamster eats bread and cake\"]"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6pisRs2HHtB",
        "colab_type": "text"
      },
      "source": [
        "### **1-1. 데이터 전처리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4ty4cVnZ4B8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "9d2906c2-4895-412f-d859-a98e59a07712"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvxjjZnBWJWj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "a7a81034-9df6-4dd8-e2d5-12f3dc361ad2"
      },
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wl = WordNetLemmatizer()\n",
        "\n",
        "# 문장 전처리\n",
        "pos_docs = []\n",
        "for line in docs_ls:\n",
        "    doc = line.split(\" \")\n",
        "    tmp_docs = []\n",
        "    for word in doc:\n",
        "        # 소문자화, Lemmatize\n",
        "        tmp_docs.append(wl.lemmatize(word.lower(), pos = 'v' or 'n'))\n",
        "    # 영어 품사 부착(PoS Tagging)\n",
        "    pos_docs.append(pos_tag(tmp_docs))\n",
        "\n",
        "pos_docs"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('cute', 'NN'), ('kitty', 'NN')],\n",
              " [('eat', 'NN'), ('rice', 'NN'), ('or', 'CC'), ('cake', 'VB')],\n",
              " [('kitty', 'NNS'), ('and', 'CC'), ('hamster', 'NN')],\n",
              " [('eat', 'NN'), ('bread', 'NN')],\n",
              " [('rice,', 'NN'), ('bread', 'NN'), ('and', 'CC'), ('cake', 'NN')],\n",
              " [('cute', 'NN'),\n",
              "  ('hamster', 'NN'),\n",
              "  ('eat', 'NN'),\n",
              "  ('bread', 'NN'),\n",
              "  ('and', 'CC'),\n",
              "  ('cake', 'NN')]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aeym2j3N-dXS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "3dcd321b-e966-413d-e45e-3282ce329c7b"
      },
      "source": [
        "# 불용어 처리(stopWord)\n",
        "stopPos = ['CC']\n",
        "stopWord = [',']\n",
        "\n",
        "docs_token = []\n",
        "tokens = []\n",
        "\n",
        "for pos_doc in pos_docs:\n",
        "    doc_token_tmp = []\n",
        "    for pos_token in pos_doc:\n",
        "        # 불용 품사 지정\n",
        "        if pos_token[1] not in stopPos:\n",
        "            # 불용어 지정\n",
        "            if pos_token[0] not in stopWord:\n",
        "                doc_token_tmp.append(pos_token[0])\n",
        "                tokens.append(pos_token[0])\n",
        "    # 문서 사용 단어\n",
        "    docs_token.append(doc_token_tmp)\n",
        "# 전체 문서 단어\n",
        "tokens = list(set(tokens))\n",
        "\n",
        "docs_token, tokens"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([['cute', 'kitty'],\n",
              "  ['eat', 'rice', 'cake'],\n",
              "  ['kitty', 'hamster'],\n",
              "  ['eat', 'bread'],\n",
              "  ['rice,', 'bread', 'cake'],\n",
              "  ['cute', 'hamster', 'eat', 'bread', 'cake']],\n",
              " ['rice', 'bread', 'cake', 'rice,', 'cute', 'kitty', 'eat', 'hamster'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiyMUQipHWEj",
        "colab_type": "text"
      },
      "source": [
        "### **1-2. LDA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTUAtmpE__Gu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "978fc35e-fec5-4986-9a2a-52a39dc8c67e"
      },
      "source": [
        "from random import randint\n",
        "\n",
        "# 토픽 랜덤 설정\n",
        "topic = 2 # 임의의 랜덤값\n",
        "topic_set = []\n",
        "\n",
        "# 토픽 랜덤 부여\n",
        "for i in range(len(docs_token)):\n",
        "    topic_count = [randint(1, topic) for a in range(len(docs_token[i]))]\n",
        "    topic_set.append(topic_count)\n",
        "\n",
        "topic_set"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 1], [2, 2, 1], [2, 1], [1, 1], [1, 2, 2], [1, 2, 2, 2, 2]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSruBwdDCg_6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24614f24-37c9-446c-9b78-3ced42161352"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 문서 내 토픽 분포\n",
        "alpha = 0.1 # 알파 값 부여\n",
        "\n",
        "topic_doc = []\n",
        "for i in range(len(topic_set)):\n",
        "    tmp = []\n",
        "    for j in range(1, topic+1):\n",
        "        if j in topic_set[i]:\n",
        "            tmp.append(topic_set[i].count(j) + alpha)\n",
        "        else:\n",
        "            tmp.append(0)\n",
        "    topic_doc.append(tmp)\n",
        "\n",
        "topic_doc"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.1, 1.1], [1.1, 2.1], [1.1, 1.1], [2.1, 0], [1.1, 2.1], [1.1, 4.1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOqC6FQLEy6u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2d058f9a-bd2d-467b-b4c4-e70893c1b932"
      },
      "source": [
        "# 토픽 내 단어 분포\n",
        "beta = 0.001 # 베타값 부여\n",
        "\n",
        "topic_word = [[0 for a in range((len(tokens)))] for b in range(topic)]\n",
        "\n",
        "for i in range(len(docs_token)):\n",
        "    for j in range(len(docs_token[i])):\n",
        "        for k in range(1, topic+1):\n",
        "            if topic_set[i][j] == k:\n",
        "                    topic_word[k-1][tokens.index(docs_token[i][j])] += 1\n",
        "\n",
        "for i in range(len(topic_word)):\n",
        "    for j in range(len(topic_word[i])):\n",
        "        topic_word[i][j] += beta\n",
        "\n",
        "topic_word"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.001, 1.001, 1.001, 1.001, 1.001, 1.001, 1.001, 1.001],\n",
              " [1.001, 2.001, 2.001, 0.001, 1.001, 1.001, 2.001, 1.001]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pTh1WFAXnh6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "58fcfdcc-58e5-4161-ea1f-59f90e2e60cb"
      },
      "source": [
        "# 합계\n",
        "prob_td = []\n",
        "prob_tw = []\n",
        "\n",
        "# 문서내 토픽 확률\n",
        "for i in range(len(topic_doc)):\n",
        "    td_total = np.sum(topic_doc[i])\n",
        "    prob_tmp = []\n",
        "    for j in range(len(topic_doc[i])):\n",
        "        prob_tmp.append(topic_doc[i][j]/td_total)\n",
        "    prob_td.append(prob_tmp)\n",
        "\n",
        "# 토픽 내 단어 합계\n",
        "for i in range(len(topic_word)):\n",
        "    tw_total = np.sum(topic_doc[i])\n",
        "    prob_tmp = []\n",
        "    for j in range(len(topic_word[i])):\n",
        "        prob_tmp.append(topic_word[i][j]/tw_total)\n",
        "    prob_tw.append(prob_tmp)\n",
        "\n",
        "prob_td, prob_tw"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[0.5, 0.5],\n",
              "  [0.34375, 0.65625],\n",
              "  [0.5, 0.5],\n",
              "  [1.0, 0.0],\n",
              "  [0.34375, 0.65625],\n",
              "  [0.2115384615384616, 0.7884615384615385]],\n",
              " [[0.00045454545454545455,\n",
              "   0.4549999999999999,\n",
              "   0.4549999999999999,\n",
              "   0.4549999999999999,\n",
              "   0.4549999999999999,\n",
              "   0.4549999999999999,\n",
              "   0.4549999999999999,\n",
              "   0.4549999999999999],\n",
              "  [0.31281249999999994,\n",
              "   0.6253124999999999,\n",
              "   0.6253124999999999,\n",
              "   0.0003125,\n",
              "   0.31281249999999994,\n",
              "   0.31281249999999994,\n",
              "   0.6253124999999999,\n",
              "   0.31281249999999994]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAw5sXewf_t3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "52cef9a6-8245-4aed-d578-15ab186c8dad"
      },
      "source": [
        "# 토픽 부여 행렬\n",
        "topic_result = []\n",
        "for i in range(len(docs_token)):\n",
        "    topic_prob = [[0 for a in range((topic))] for a in range(len(docs_token[i]))]\n",
        "    topic_result.append(topic_prob)\n",
        "topic_result\n",
        "\n",
        "# LDA 계산\n",
        "for i in range(len(topic_result)):\n",
        "    for j in range(len(topic_result[i])):\n",
        "        for k in range(topic):\n",
        "                topic_result[i][j][k] = prob_td[i][k] * prob_tw[k][tokens.index(docs_token[i][j])]\n",
        "\n",
        "topic_result"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[0.22749999999999995, 0.15640624999999997],\n",
              "  [0.22749999999999995, 0.15640624999999997]],\n",
              " [[0.15640624999999997, 0.410361328125],\n",
              "  [0.00015625, 0.20528320312499995],\n",
              "  [0.15640624999999997, 0.410361328125]],\n",
              " [[0.22749999999999995, 0.15640624999999997],\n",
              "  [0.22749999999999995, 0.15640624999999997]],\n",
              " [[0.4549999999999999, 0.0], [0.4549999999999999, 0.0]],\n",
              " [[0.15640624999999997, 0.000205078125],\n",
              "  [0.15640624999999997, 0.410361328125],\n",
              "  [0.15640624999999997, 0.410361328125]],\n",
              " [[0.09625, 0.24664062499999997],\n",
              "  [0.09625, 0.24664062499999997],\n",
              "  [0.09625, 0.4930348557692308],\n",
              "  [0.09625, 0.4930348557692308],\n",
              "  [0.09625, 0.4930348557692308]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o5xCwiIgfPh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84a7ca34-6b84-4fe3-937c-fcd1757fb1de"
      },
      "source": [
        "# 최종 토픽 할당\n",
        "LDA_result = []\n",
        "for i in range(len(docs_token)):\n",
        "    LDA_result.append([[0 for a in range((topic))] for a in range(len(docs_token[i]))])\n",
        "\n",
        "for i in range(len(topic_result)):\n",
        "    for j in range(len(topic_result[i])):\n",
        "        LDA_result[i][j] = topic_result[i][j].index(np.max(topic_result[i][j])) + 1\n",
        "\n",
        "LDA_result"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1], [2, 2, 2], [1, 1], [1, 1], [1, 2, 2], [2, 2, 2, 2, 2]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xlfLLMLovHe",
        "colab_type": "text"
      },
      "source": [
        "## **2. 잠재 디리클레 할당(LDA) 클래스화**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAOtjtlYxKU0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "1da3a140-48de-4b57-c919-1d0e727e10df"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvQaU0Otp3b6",
        "colab_type": "text"
      },
      "source": [
        "### **2-1. LDA 클래스**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD5QO4h9nMmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import randint\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class LDA():\n",
        "    def __init__(self, topic):\n",
        "        self.topic = topic\n",
        "        self.pos_docs = []\n",
        "        self.docs_token = []\n",
        "        self.tokens = []\n",
        "        self.topic_set = []\n",
        "        self.topic_doc = []\n",
        "        self.topic_word = []\n",
        "        self.prob_td = []\n",
        "        self.prob_tw = []\n",
        "        self.topic_result = []\n",
        "        self.LDA_result = []\n",
        "        self.result_word = []\n",
        "        self.result_topic = []\n",
        "\n",
        "        # 불용어, 불용품사\n",
        "        self.stopPos = ['CC']\n",
        "        self.stopWord = [',']\n",
        "\n",
        "    # 문장 전처리\n",
        "    def preprocessing_data(self, docs_ls):\n",
        "        wl = WordNetLemmatizer()\n",
        "        for line in docs_ls:\n",
        "            doc = line.split(\" \")\n",
        "            tmp_docs = []\n",
        "            for word in doc:\n",
        "                # 소문자화, Lemmatize\n",
        "                tmp_docs.append(wl.lemmatize(word.lower(), pos = 'v' or 'n'))\n",
        "            # 영어 품사 부착(PoS Tagging)\n",
        "            self.pos_docs.append(pos_tag(tmp_docs))\n",
        "\n",
        "        return self.pos_docs\n",
        "\n",
        "    # 토큰화\n",
        "    def tokenize(self):\n",
        "        for pos_doc in self.pos_docs:\n",
        "            doc_token_tmp = []\n",
        "            for pos_token in pos_doc:\n",
        "                # 불용 품사 지정\n",
        "                if pos_token[1] not in self.stopPos:\n",
        "                    # 불용어 지정\n",
        "                    if pos_token[0] not in self.stopWord:\n",
        "                        doc_token_tmp.append(pos_token[0])\n",
        "                        self.tokens.append(pos_token[0])\n",
        "                # 문서 사용 단어\n",
        "            self.docs_token.append(doc_token_tmp)\n",
        "            # 전체 문서 단어\n",
        "        self.tokens = list(set(self.tokens))\n",
        "\n",
        "        return self.docs_token, self.tokens\n",
        "\n",
        "    # 토픽 설정\n",
        "    def give_topic(self):\n",
        "        for i in range(len(self.docs_token)):\n",
        "            topic_count = [randint(1, self.topic) for a in range(len(self.docs_token[i]))]\n",
        "            self.topic_set.append(topic_count)\n",
        "\n",
        "        return self.topic_set\n",
        "\n",
        "    # 문서 내 토픽 분포\n",
        "    def chk_topicdoc(self, alpha):\n",
        "        for i in range(len(self.topic_set)):\n",
        "            tmp = []\n",
        "            for j in range(1, self.topic+1):\n",
        "                if j in self.topic_set[i]:\n",
        "                    tmp.append(self.topic_set[i].count(j) + alpha)\n",
        "                else:\n",
        "                    tmp.append(0)\n",
        "            self.topic_doc.append(tmp)\n",
        "\n",
        "        return self.topic_doc\n",
        "\n",
        "    # 토픽 내 단어 분포\n",
        "    def chk_topicword(self, beta):\n",
        "        self.topic_word = [[0 for a in range((len(self.tokens)))] for b in range(self.topic)]\n",
        "        for i in range(len(self.docs_token)):\n",
        "            for j in range(len(self.docs_token[i])):\n",
        "                for k in range(1, self.topic+1):\n",
        "                    if self.topic_set[i][j] == k:\n",
        "                            self.topic_word[k-1][self.tokens.index(self.docs_token[i][j])] += 1\n",
        "        for i in range(len(self.topic_word)):\n",
        "            for j in range(len(self.topic_word[i])):\n",
        "                self.topic_word[i][j] += beta\n",
        "\n",
        "        return self.topic_word\n",
        "\n",
        "    # 문서 내 토픽, 토픽 내 단어 확률\n",
        "    def cal_probabilty(self):\n",
        "        for i in range(len(self.topic_doc)):\n",
        "            td_total = np.sum(self.topic_doc[i])\n",
        "            prob_tmp = []\n",
        "            for j in range(len(self.topic_doc[i])):\n",
        "                prob_tmp.append(self.topic_doc[i][j]/td_total)\n",
        "            self.prob_td.append(prob_tmp)\n",
        "        # 토픽 내 단어 합계\n",
        "        for i in range(len(self.topic_word)):\n",
        "            tw_total = np.sum(self.topic_doc[i])\n",
        "            prob_tmp = []\n",
        "            for j in range(len(self.topic_word[i])):\n",
        "                prob_tmp.append(self.topic_word[i][j]/tw_total)\n",
        "            self.prob_tw.append(prob_tmp)\n",
        "\n",
        "        return self.prob_td, self.prob_tw\n",
        "\n",
        "    # 토픽 부여 행렬\n",
        "    def cal_LDA(self):\n",
        "        # LDA 행렬 생성\n",
        "        for i in range(len(self.docs_token)):\n",
        "            topic_prob = [[0 for a in range((self.topic))] for a in range(len(self.docs_token[i]))]\n",
        "            self.topic_result.append(topic_prob)\n",
        "        self.topic_result\n",
        "        # LDA 계산\n",
        "        for i in range(len(self.topic_result)):\n",
        "            for j in range(len(self.topic_result[i])):\n",
        "                for k in range(self.topic):\n",
        "                        self.topic_result[i][j][k] = self.prob_td[i][k] * self.prob_tw[k][self.tokens.index(self.docs_token[i][j])]\n",
        "\n",
        "        return self.topic_result\n",
        "    \n",
        "    # 최종 토픽 할당\n",
        "    def result_LDA(self):\n",
        "        for i in range(len(self.docs_token)):\n",
        "            self.LDA_result.append([[0 for a in range((self.topic))] for a in range(len(self.docs_token[i]))])\n",
        "        for i in range(len(self.topic_result)):\n",
        "            for j in range(len(self.topic_result[i])):\n",
        "                self.LDA_result[i][j] = self.topic_result[i][j].index(np.max(self.topic_result[i][j])) + 1\n",
        "        # 결과 출력\n",
        "        for i in range(len(self.topic_result)):\n",
        "            for j in range(len(self.topic_result[i])):\n",
        "                self.result_word.append(self.docs_token[i][j])\n",
        "                self.result_topic.append(self.LDA_result[i][j])\n",
        "\n",
        "    # 자동 실행\n",
        "    def run(self, docs_ls, alpha, beta):\n",
        "        self.preprocessing_data(docs_ls)\n",
        "        self.tokenize()\n",
        "        self.give_topic()\n",
        "        self.chk_topicdoc(alpha)\n",
        "        self.chk_topicword(beta)\n",
        "        self.cal_probabilty()\n",
        "        self.cal_LDA()\n",
        "        self.result_LDA()\n",
        "        \n",
        "        return pd.DataFrame([self.result_topic], columns=self.result_word)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyglW_Agp9km",
        "colab_type": "text"
      },
      "source": [
        "### **2-1. LDA 클래스 결과 확인**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZXxmHnZpsfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LDA(3)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkFuRU5Jp2QD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs_ls = [\"Cute kitty\",\n",
        "          \"Eat rice or cake\",\n",
        "          \"Kitty and hamster\",\n",
        "          \"Eat bread\",\n",
        "          \"Rice, bread and cake\",\n",
        "          \"Cute hamster eats bread and cake\",\n",
        "          \"Dog eat cake\"\n",
        "          \"Dog and hamster eat bread\"\n",
        "          ]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eUlPR86uiA2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "07731287-15ba-4599-e6c8-48bece9493aa"
      },
      "source": [
        "lda.run(docs_ls, 0.1, 0.001)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cute</th>\n",
              "      <th>kitty</th>\n",
              "      <th>eat</th>\n",
              "      <th>rice</th>\n",
              "      <th>cake</th>\n",
              "      <th>kitty</th>\n",
              "      <th>hamster</th>\n",
              "      <th>eat</th>\n",
              "      <th>bread</th>\n",
              "      <th>rice,</th>\n",
              "      <th>bread</th>\n",
              "      <th>cake</th>\n",
              "      <th>cute</th>\n",
              "      <th>hamster</th>\n",
              "      <th>eat</th>\n",
              "      <th>bread</th>\n",
              "      <th>cake</th>\n",
              "      <th>dog</th>\n",
              "      <th>eat</th>\n",
              "      <th>cakedog</th>\n",
              "      <th>hamster</th>\n",
              "      <th>eat</th>\n",
              "      <th>bread</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   cute  kitty  eat  rice  cake  kitty  ...  dog  eat  cakedog  hamster  eat  bread\n",
              "0     3      3    1     2     3      3  ...    1    1        2        1    1      1\n",
              "\n",
              "[1 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}